# Triangulation Matting: Foregound and Alphamatte extraction for Still Images and Videos

This repository does triangulation matting using blue and green background images (and videos). Two images of the same object are taken with different backgrounds, blue and green. This is then used along with two blank images of same background to **infer values of the alphamask and foreground**. The alphamask are nothing but weights on the values of the foreground and can be used to insert that foreground on any other background.

This work was originally done towards a final project in a Computer Vision course. The goal back then was to take videos generated by another team member (who used team fortress 2 and the source filmmaker) to capture footage (over a green and blue background). I wrote the code in C++ to do triangulation matting. A third person used the alphmask to blend the foreground onto the background.

As in the original work and (for this excercise), we're only interested in extracting the foreground and alphamask (or alphamatte). We can do this for every frame of video files (`Video Matting`) or over still images (`Still Image Matting`).

**`Within this context and the original purpose of this codebase, that means that it can support the creation of alphamask and foreground estimation, over still images and video, given capture of a foreground in still or moving form over different backgrounds, e.g. green and blue. Although anyone is free to use the alphamask and extracted foreground to blend the end results on any background of their choice.`**

All images and videos processed focus on this functionality (alphamask and foreground extraction) and the results reflect as such.

**Before moving forward, please consider looking at the appendix to get a super quick overview of related concepts, if you're so interested; otherwise, feel free to read on.
[Please visit the appendix here, if you wanted to](#Appendix).**

If there's such an alphamask or a *single channel per pixel floating point image*, `alpha` which can properly capture the presence of the `foreground` (per pixel) in terms of floating point weights between zero and one, then it can be used to split an image into `foreground` and `background` by using it's original values and inverse values, i.e. `(1-alpha)`. Doing this over two differently colored backgrounds (green and blue) it can be represented as:

![matting equation](https://i.imgur.com/9PTRsJj.png)

where any `foreground_blue` is the foreground captured with the blue background. the `emptyBackground_blue` is the same scene without the foreground and only the background color. `foreground_ind` is the extracted foreground which is present without any background. `alpha` is the alphamask or alphamatte.  The equations are same for green images. Here we're talking at a pixel level and considering all the channels of an image per pixel, i.e. Red, Green and Blue.

the above on being further solved, leads to a set of linear equations which can be represented as:

![matting](https://user-images.githubusercontent.com/28497335/138963602-8a587a14-66e9-44fa-8e72-17e2481bdc2a.png)

(Credit to [brown university](http://cs.brown.edu/courses/cs129/results/final/njooma/) for the above visual)

To read more on the above, please look at the [appendix](#Appendix).

This codebase has the capability to independently process videos (read and write them using a custom Video class) and perform this matting on still images and each and every frame of a video.

The original green and blue background videos which were originally used a long time ago are sadly lost :( but two artificial videos have been created
from the still (foreground with relevant, blue or green background color) images that were still present. These artificial videos are simply repetitions of the same image over a thirty second period, sampled at 30 frames per second. When matting is run using them though, due to certain loss in image quality when performing the image to video conversion, the actual alpha matte and matted frame (foreground) from the per frame of the video vary from when those same still images (that were used to produce the videos) are independently used to create the matte and foreground. This is very interesting and you can even see this when you try out the tests.

Video files may be very slow to process depending on your system which is why the scale parameter has been provided for (faster) testing purposes.

OpenCV is still seemingly limited (or at least is a bit problematic in) writing anything (in terms of the `VideoWriter` object writing the videos) other than ".avi" out of the box (if you install from the repo), so please use avi formats to write output files.

## Installation


1. Install dependencies as seen [here](https://linuxize.com/post/how-to-install-opencv-on-ubuntu-18-04/)
2. Install openCV: `sudo apt install libopencv libopencv-dev`, this codebase requires **OpenCV version 4**.
3. Windows users see [this](https://learnopencv.com/install-opencv-on-windows/) or use chocolatey. Mac users see [this](https://www.pyimagesearch.com/2018/08/17/install-opencv-4-on-macos/).
4. Install ffmpeg: `sudo apt install ffmpeg`. Windows users, please see [this](https://www.wikihow.com/Install-FFmpeg-on-Windows). Mac users, [this](http://jollejolles.com/install-ffmpeg-on-mac-os-x/).
5. Clone this repo: `git clone https://github.com/hoffsupes/Triangulation-Matting.git`


## Usage

### Still Image Matting

1. To perform still image matting, traverse to the root of the project directory and run:

```
g++ src/main.cpp src/image.cpp src/video.cpp src/video_matte_applier.cpp src/matte_applier.cpp  -Iinclude -o bin/main `pkg-config --cflags --libs opencv4`;
```
2. Next run the `main` binary which has been created in the `bin/` folder.

```      
./bin/main     image-matte \
               path_to_blue_image \
               path_to_green_image \
               path_to_blue_still_image \
               path_to_green_still_image \
               path_to_foreground_image \
               path_to_alpha_image \
```

The above options are explained as follows:

1. **path_to_blue_image**: Relative path to the blue background image which is input
2. **path_to_green_image**: Relative path to the green background image which is input
3. **path_to_blue_still_image**: Relative path to the blank blue still image which is input
4. **path_to_green_still_image**: Relative path to the blank green still image which is input
5. **path_to_foreground_image**: Relative path to the foreground image to be output
6. **path_to_alpha_image**: Relative path to the alpha mask image to be output

Example:

```
./bin/main     image-matte \
               data/b0.png \
               data/g0.png \
               data/b0_blank.png \
                 data/g0_blank.png  \
               data/mattedImage.png \
               data/finalMask.png
```

### Video Matting

1. To perform video matting, traverse to the root of the project directory and run:

```
g++ src/main.cpp src/image.cpp src/video.cpp src/video_matte_applier.cpp src/matte_applier.cpp  -Iinclude -o bin/main `pkg-config --cflags --libs opencv4`;
```
2. Next run the `main` binary which has been created in the `bin/` folder.

```      
./bin/main     video-matte \
               path_to_blue_video \
               path_to_green_video \
               path_to_blue_still_image \
               path_to_green_still_image \
               path_to_foreground_image_folder \
               path_to_alpha_image_folder \
               mattedvideopath \
               image_scaling_value_for_faster_processing \
               0_or_1_for_video_display_only \
               0_or_1_to_display_output_or_not
```

The above options are explained as follows:

1. **path_to_blue_video**: Relative path to the blue background video which is input
2. **path_to_green_video**: Relative path to the green background video which is input
3. **path_to_blue_still_image**: Relative path to the blank blue still image which is input
4. **path_to_green_still_image**: Relative path to the blank green still image which is input
5. **path_to_foreground_image_folder**: Relative path to the foreground image folder to be output to, please dont include a '/' at the end. That is use `data/Foreground` instead of `data/Foregound/`
6. **path_to_alpha_image_folder**: Relative path to the alpha mask image folder to be output to, please dont include a '/' at the end. That is use `data/Alpha` instead of `data/Alpha/`
7. **mattedvideopath**: Output matted video path
8. **image_scaling_value_for_faster_processing**: Image scaling value, since matrix inverse per pixel is very computationally extensive, if you really wanted to test the whole video, or see this in action (!). Please use a lower value (range is between 0 - 1 ) e.g. 0.3 and this will scale or resize (downsample) your images down to a smaller size as per the scale given before doing trimat (the image intensity estimation process is even worse for something like Trimatting, which actually needs fine high quality per pixel values to give clean mattes -- you can see this if you try out `make matte-test` in the root directory to run the `matte_test.cpp` or look at the pre-rendered example images in the `data` folder) but this should suffice since a) the video data itself is only for testing b) this is only present should you want to see the code run to completion.
9. **0_or_1_for_video_display_only**: `0` or `1` numerical values, denoting `true` or `false` values to denote if you want to output (write output files) only videos or the intermediate files, foreground and alpha mask as well
10. **0_or_1_to_display_output_or_not**: `0` or `1` numerical values, denoting `true` or `false` values to denote if you want to display (i.e. display output of video frames as they are being processed) the matte and foreground images (in concatenated fashion) or not

Example:

```
./bin/main    video-matte \
              data/Videos/blue_.mp4  \
              data/Videos/green_.mp4 \
              data/b0.png   \
              data/g0.png   \
              data/Foreground  \
              data/Alpha    \
              data/Videos/final_matte.avi \
              0.01 0 1
```

## Test Suites

Makefile tests are included, to run them traverse to the root of the directory and execute accordingly as given below:

1. `make test-all`:
    Run complete test suite
2. `make video_matte-test`:
    To test the `VideoMatte` capabilties
3. `make video-test`:
    To test `Video` reading and writing
4. `make matte-test`:
    To test `Matte` application on an `Image`
5. `make image-test`:
    To test the capabilies of the `Image` class
6. `make utilities-test`:
    Test capabilites of the `miniUtilities` class
7. `make tester-test`:
    Test capabilites of the `tester` class
8. `make main-test`:
    Test main.cpp
9. `make clean`:
    Cleans everything up, deletes content of `Foreground` and `Alpha` folder and even the `bin` folder

## Directory structure

1. `bin/`: Contains all the binary files
2. `data/`: Contains image files and subfolders `Videos`, `Foregound`, `Alpha` which contain other image and video files
3. `include/trimat`: Contains the header (class definition) in `.h` files
4. `src`: Contains member functions in `.cpp` files
5. `tests`: Contains all the tests

## Classes

I've modeled my own very simple `Image` class which is used by a `Video` class. The ``Video`` class has capabilities to very simply read and write videos all in one place.
That includes releasing the actual object files after use which is very important in a language like C++. I'm also create a Matting (`Matte`) class which relies on the `Image` class to do the Matting. The Matting class is then extended to a `VideoMatte` class (which inherits all the properties of the Matting class) that adds capability to read the blue and green background videos, and write the resultant video (composed of the alphamatte and the foreground image per frame); utilizing the capabilities of the `Video` class. Although point to note, that this is in no way a critique of the openCV library which absolutely amazing and this work only seeks to make the methods which would typically be relevant (or more commonly used in regards to it) to this specific problem (i.e. video matting) simpler to use and encapsulated closely with the data it would work on. The generalizability of OpenCV is what makes it quite powerful in terms of the massive number of features it offers and all I've done is collected some of the features together for common use.

Obviously, OpenCV (especially, the `Mat` class and other accompagnying ones which operate on `Mat`) is highly feature rich (as it covers a lot more immensity in terms of capabilities offered across a wide variety of computer vision and even separate pattern recognition tasks, e.g. means to train classical machine learning models on data which might not have anything to do with computer vision, even if not every capability is not present directly within the `Mat` class -- there's nothing wrong with that, as that allows for near perfect flexibility when using it for other applications) and these are only a select few capabilities which seemed most pertinent and as it seemed that some additional capabilities of simplicity could be brought in for certain methods (i.e. as compared to using them out of the box from OpenCV vs modified here so that repeated use becomes easier).

An architecture diagram can be seen below, followed by a brief explanation of each class:

![VideoMatter(2)](https://user-images.githubusercontent.com/28497335/139788267-1ce5df4d-2b57-46c1-9408-94341650979e.png)

1. `Image`: Creates a simple image class, which provides relatively easier access for commonly used OpenCV image functions, e.g. `imshow` is bundled in with the image container rather than separate, images can be easily allocated from files very simply etc.
2. `Video`: Models OpenCV's `VideoCapture` and `VideoWriter` in one place and simple method calls replace complicated allocation (e.g. `VideoCapture capture;`) and deallocation (`capture.release();`) and it take cares of some other things behind the scenes to make their use simple and painless (e.g. reading videos is much simpler, at least better than typing in massive OpenCV VideoCapture or VideoWriter Properties like CAP_PROP_WHITE_BALANCE_BLUE_U, to just quickly get a property or read one frame) as well.
3. `Matte`: Does matting using on `Image` objects, i.e. alphamask and foreground extraction
4. `VideoMatte`: Extends capabilities of the `Matte` class to handle Videos. Has functionality to contain green, blue and even writing the resultant videos.
5. `tester`: Provides simple testing capabilities
6. `miniUtilities`: Contains simple string utilities related to filehandling

## Appendix

This section is

### Foreground and Images

A foreground within an image can be any object or person of interest whom we seek to insert over a separate background or seek to extract, traverse or label for any reason. An image here can be defined as a 2D matrix of intensities; where the whole image can be thought of composed of these "dots" or pixels which are nothing but numerical values. This can be single numerical values per pixel or square dot (e.g. in a grayscale image) or a three numrical color values per pixel (e.g. in a color RGB image).

### Segmentation

A very simple way to extract this foreground from an image could be segmenting intentisty values over an image. Very simple segmenting involves nothing but setting any regions of intensity within the image which are greater or lesser than some numerical threshold as one (or zero) e.g. setting all values in a greayscale image (with an intensity range of 0-255) as one if the values are greater than 128. The goal being to "segment" or focus out one region out from the image. This can be for something as trivial as localizing or even extracting an object from the image, if the "binary mask" of zeros and ones are clean (to contain only that object).

So if that object (with a clean binary mask) we're segmenting is a foreground object then we can do foreground extraction using segmenting (i.e. where-ever the binary mask is one, that's a foreground (FG) region in the image, whereever it is zero in the mask, then it is not). This can work say when you're capturing a dark object over a bright background and one knows that any intenstiy value lesser than a certain value is the foreground (or object of interest).

But this approach is typically not that robust if one wanted to obtain a clean cut of the foreground (as getting a clean cut of that original segented object can be hard) and there can be "noisy" regions, which are essentially not FG regions but still satisfy the thresholding condition defined earlier. Even if these regions are removable through image processing techniques when they're properly separated from the object of interest and easily separable from it (this can happen say in case of something like salt and pepper noise over the image or speckles of little dots spread over the whole image and there are known methods to remove the same), sometimes they're not! Moreso, segmenting is not really that good for soft or "fuzzy" edges or localizing objects like feathers which might have regions at the edges which might be to some degree, part foreground and background (at the same time), or for motion blur, handling semi-transparent objects, and not really robust to changes in illumination over the image (assuming we're attempting to do this without any additional image processing).

### Matting

Hence a more robust way to offput the above inconsitencies was that of attempting to place the foreground on a linear, homogenous background with a color which is easy to differentiate from the foreground. This is in essence what matting does. Single color matting techniques like green or blue screen matting would

Matting or keying is the problem of estimating a given foreground and the alphamask which when combined with the foreground and new background can be used to composite that new foreground onto a new background. We don't segment or threshold values, rather we solve an equation at every pixel (at most three equations with three unknowns but with certain assumtions there are lesser equations) to obtain the values of the FG and alphamask (assuming that the color within the background does not exist within the foreground at all, e.g. greenscreen matting assumes that green does not exist within the foreground). The alphamask is nothing but a similar binary mask, **except** that now every dot or pixel has capability to be both foreground and background at the same time, i.e. values range from zero to one instead of only zero and one (here, one implies opaque while zero implies transparent).

Practically, green screen matting can be easily achieved by shooting infront of a green colored background (called a *green screen* hence the name) and you might even notice in studio's (especially in *making of* of various holywood blockbusters) people shooting infront of green screens. That's the foreground (actors) being shot infront of a green screen so that a different background can be inserted later.

The problem with green screen or even blue screen matting is the lack of capability to handle shadows, color of the background existing in the foregreound, and even in studios particularly, the illumination from the screen "spills" the background color on the foreground.

This is where triangulation matting comes in. It does the same (overlaying of foreground over some constant color) but over two colors instead of one. This leads to six equations instead of three and four unknowns.


##### Feedback

###### If you notice any inconsitencies or have any input to provide, feel free to reach out to me at dassgaurav93@gmail.com
