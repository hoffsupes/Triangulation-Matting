# Triangulation Matting: Foregound and Alphamatte extraction for Still Images and Videos

This repository does triangulation matting using blue and green background images (and videos). Two images of the same object are taken with different backgrounds, blue and green (although they can be different colors). This is then used along with two blank images of same background to **infer values of the alphamask and foreground**. The alphamask are nothing but weights on the values of the foreground and can be used to insert that foreground on any other background.

This work was originally done towards a final project in a Computer Vision course. The goal back then was to take videos generated by another team member (who used team fortress 2 and the source filmmaker) to capture footage (over green and blue backgrounds, this also produced some test data in form of images for me which you see in the repo). My contribution was using C++ and OpenCV to calculate the alphamatte (and the foreground obtained in the process). I recently picked this up again and refactored the work.

As in the original work and (for this excercise), we're only interested in extracting the foreground and alphamask (or alphamatte). We can do this for every frame of video files (`Video Matting`) or over still images (`Still Image Matting`).

**Within this context and given the original purpose of this codebase, it solves a system of linear equations born of the idea of triangulation matting (by Smith and Blinn, 96) that leads to the estimation of the alphamask and foreground over still images and video, given the various images and videos of a foreground captured over different backgrounds, e.g. green and blue.**

**Anyone is free to use the alphamask and extracted foreground to blend the end results on any background of their choice. Although any compositing and blending (onto a new background) is not covered within this codebase, and only the estimation of alphamatte and foreground is done.**

All images and videos processed focus on this functionality (alphamask and foreground extraction) and the results reflect as such.

**Before moving forward, please consider looking at the [appendix](#Appendix) to get a super quick overview of related concepts, if you're so interested; otherwise, please feel free to read on.**

## Triangulation Matting
<br />
<p align="center">
<img src="https://www.researchgate.net/profile/Alvy-Smith/publication/220720513/figure/fig1/AS:340320320540690@1458150095674/Ideal-triangulation-matting-a-Object-against-known-constant-blue-b-Against-constant.png" width=50% height=50%><br />
</p>
<br />

If there's such an alphamask or a *single channel per pixel floating point image*, `alpha` which can properly capture the presence of the `foreground` (per pixel) in terms of floating point weights between zero and one, then it can be used to split an image into `foreground` and `background` by using it's original values (`alpha`) and inverse values, i.e. `(1-alpha)`. Doing this over two differently colored backgrounds (green and blue) it can be represented as:

![matting equation](https://i.imgur.com/9PTRsJj.png)

where any `foreground_blue` is the foreground captured with the blue background. the `emptyBackground_blue` is the same scene without the foreground and only the background color. `foreground_ind` is the extracted foreground which is present without any background. `alpha` is the alphamask or alphamatte.  The equations are same for green images. Here we're talking at a pixel level and considering all the channels of an image per pixel, i.e. Red, Green and Blue. The  above equations, on being further expanded and solved, lead to a system of linear equations which can be represented as:

![matting](https://user-images.githubusercontent.com/28497335/138963602-8a587a14-66e9-44fa-8e72-17e2481bdc2a.png)

The `R_o, B_o, G_o` and `alpha` on the right hand side, in the vector represent the unknowns to be solved in this system of linear equations. Each instance of this equation is solved at every pixel; that is, this system of equations is constructed **once per pixel** using the four blue and green (or any **two** different backgrounds) images, which are:- two blue and green backgrounded images which contain the foreground and two blank (no foreground) blue and green background only images (blue and green). After putting that together (which involves the creation of a 4 column matrix on the right and the 6 element vector on the left), the calculation is done to solve for one value of the alphamatte (`alpha`) along with the foreground (`R_o, B_o, G_o`) at that given pixel. There can be improvements made on this if needed, e.g. doing a segmentation of the foreground and dilating (expanding) it to a great degree and then doing calculations across all images only on that segmented region.  

A similar schema applies to video (i.e. 2 videos for the foreground containing images, and two background only still images, since background in this case was artificial and remained the same and since the whole nature of the data used in this endeavor even before this was artificial), but processing is done consequtively in tandem across all images and videos **per frame**.

This codebase has the capability to independently process videos (read and write them using a custom `Video` class) and perform this matting on still images and each and every frame of a video.

The original green and blue background videos which were originally used a long time ago are sadly lost :( but two artificial videos have been created
from the still (foreground with relevant, blue or green background color) images that were still present. These artificial videos are simply repetitions of the same image over a thirty second period, sampled at 30 frames per second.

Video files may be very slow to process depending on your system which is why the scale parameter has been provided for (faster) testing purposes.

OpenCV is still seemingly limited (or at least is a bit problematic in) writing anything (in terms of the `VideoWriter` object writing the videos) other than ".avi" out of the box (if you install from the repo -- I'm doing so on a variant of Ubuntu 20.04.2 LTS), so please use avi formats to write output files.

## Prerequisites

To run this codebase you will need:

1. OpenCV >= 4.0
2. gcc/g++ == 9.3.0

## Installation

Assumption here is that you're running a debian based linux system, but I do cover how you can install some components for Windows and Mac. For those on arch based distros you would have to figure out how to install the packages (mentioned below) on your end.

1. Install dependencies as seen [here](https://linuxize.com/post/how-to-install-opencv-on-ubuntu-18-04/)
2. Install openCV: `sudo apt install libopencv libopencv-dev`, this codebase requires **OpenCV version 4**.
3. Windows users see [this](https://learnopencv.com/install-opencv-on-windows/) or use chocolatey. Mac users see [this](https://www.pyimagesearch.com/2018/08/17/install-opencv-4-on-macos/).
4. Install ffmpeg: `sudo apt install ffmpeg`. Windows users, please see [this](https://www.wikihow.com/Install-FFmpeg-on-Windows). Mac users, [this](http://jollejolles.com/install-ffmpeg-on-mac-os-x/).
5. Clone this repo: `git clone https://github.com/hoffsupes/Triangulation-Matting.git`


## Usage

### Still Image Matting

This operation estimates the `alpha`matte (and `foreground`) given RGB images of the following:-

- Foregound captured over a green background
- Foregound captured over a blue background
- Only the green background
- Only the blue background

To use the above to perform the given `alpha` and `foreground` estimation, please go through the following steps:

1. To perform still image matting (or foreground and alphamask extraction over still images), traverse to the root of the project directory and run:

```
g++ src/main.cpp src/image.cpp src/video.cpp src/video_matte_applier.cpp src/matte_applier.cpp  -Iinclude -o bin/main `pkg-config --cflags --libs opencv4`;
```
2. Next run the `main` binary which has been created in the `bin/` folder.

```      
./bin/main     image-matte \
               path_to_blue_image \
               path_to_green_image \
               path_to_blue_still_image \
               path_to_green_still_image \
               path_to_foreground_image \
               path_to_alpha_image \
```

The above options are explained as follows:

1. **path_to_blue_image**: Relative path to the blue background image (that has both the foreground and background) which is input. This contributes to the system of linear equations seen above and helps in solving for the `alpha` and `foreground` values.
2. **path_to_green_image**: Relative path to the green background image (that has both the foreground and background) which is input. This contributes to the system of linear equations seen above and helps in solving for the `alpha` and `foreground` values.
3. **path_to_blue_still_image**: Relative path to the blank blue still image (only background) which is input. This contributes to the system of linear equations seen above and helps in solving for the `alpha` and `foreground` values.
4. **path_to_green_still_image**: Relative path to the blank green still image (only background) which is input. This contributes to the system of linear equations seen above and helps in solving for the `alpha` and `foreground` values.
5. **path_to_foreground_image**: Relative path to the `foreground` image to be output
6. **path_to_alpha_image**: Relative path to the `alpha` mask (alphamatte) image to be output
7. **image-matte**: The actual parameter option given to denote that still images are being operated upon and is a means to use this mode.

Example:

```
./bin/main     image-matte \
               data/b0.png \
               data/g0.png \
               data/b0_blank.png \
                 data/g0_blank.png  \
               data/mattedImage.png \
               data/finalMask.png
```

In the above example, on feeding in `b0.png`,`g0.png`,`b0_blank.png`,`g0_blank.png`; one can obtain the `alpha` as `finalMask.png` and `foreground` as `mattedImage.png`.

### Video Matting

This operation estimates the `alpha` (and `foreground`) video (currently in AVI format) given RGB videos (can be in mp4 format) of the following:-

- Foregound captured over a green background
- Foregound captured over a blue background
- Only the green background
- Only the blue background

Additionally there might be necessity for certain directories to store any intermediate results of per frame processing of videos.

To use the above to perform the given `alpha` and `foreground` estimation, please go through the following steps:

1. To perform video matting (or foreground and alphamask extraction over videos), traverse to the root of the project directory and run:

```
g++ src/main.cpp src/image.cpp src/video.cpp src/video_matte_applier.cpp src/matte_applier.cpp  -Iinclude -o bin/main `pkg-config --cflags --libs opencv4`;
```
2. Next run the `main` binary which has been created in the `bin/` folder.

```      
./bin/main     video-matte \
               path_to_blue_video \
               path_to_green_video \
               path_to_blue_still_image \
               path_to_green_still_image \
               path_to_foreground_image_folder \
               path_to_alpha_image_folder \
               mattedvideopath \
               image_scaling_value_for_faster_processing \
               0_or_1_for_video_output_only \
               0_or_1_to_display_output_or_not
```

The above options are explained as follows:

1. **path_to_blue_video**: Relative path to the blue background video (that has both the foreground and background) which is input. Individual frames from this video contribute to the system of linear equations that are seen above and help in solving for the `alpha` and `foreground` values.
2. **path_to_green_video**: Relative path to the green background video (that has both the foreground and background) which is input. Individual frames from this video contribute to the system of linear equations that are seen above and help in solving for the `alpha` and `foreground` values.
3. **path_to_blue_still_image**: Relative path to the blank blue still image (only background) which is input. This contributes to the system of linear equations that areseen above and helps in solving for the `alpha` and `foreground` values.
4. **path_to_green_still_image**: Relative path to the blank green still image (only background) which is input. This contributes to the system of linear equations that areseen above and helps in solving for the `alpha` and `foreground` values.
5. **path_to_foreground_image_folder**: Relative path to the foreground image folder to be output to. It will output the `foreground` per frame of the video, to this folder.  Please dont include a '/' at the end when feeding in the path to the folder, e.g. use `data/Foreground` instead of `data/Foregound/`.
6. **path_to_alpha_image_folder**: Relative path to the alpha mask image folder to be output to. It will output the estimated `alpha` per frame of the video file, in this folder.  Please dont include a '/' at the end when feeding in the path to the folder, e.g. use `data/Alpha` instead of `data/Alpha/`
7. **mattedvideopath**: Output matted video path. Right now matted video output is just the foreground obtained per frame, i.e. a video composed of `foreground` obtained per frame.
8. **image_scaling_value_for_faster_processing**: Image scaling value, since matrix inverse per pixel is very computationally extensive (to solve system of linear equations you have to take inverse of the four column matrix seen before in ![this section](#Triangulation Matting)), if you really wanted to test the whole video, or see this in action (!), then please use a lower ranged value (range is between 0 - 1 ) e.g. 0.3 and this will scale or resize (downsample) your images down to a smaller size before solving for the alphamask and foreground estimation. This is okay since a) the video data and even code itself is only for testing b) this is only present should you want to see the code run to completion. There can be improvements made to account for this (long processing time), e.g. detecting where the foreground is using simple segmentation first. dilating the binary mask and then only doing these calculations per pixel for the limited area within the segmented region within the image; as opposed to the whole image.
9. **0_or_1_for_video_output_only**: `1` or `0` numerical values, denoting `true` or `false` values to denote if you want to write output files for only the end video (right now, the matted video is just the `foreground` obtained per frame) or all the intermediate files in the folders given above, `foreground` and `alpha` mask as well.
10. **0_or_1_to_display_output_or_not**: `1` or `0` numerical values, denoting `true` or `false` values to denote if you want to display (i.e. display output of video frames as they are being processed) the `alpha` and `foreground` images (in concatenated fashion) or not
11. **video-matte**: The option that allows access to the video matting capabilities

Example:

```
./bin/main    video-matte \
              data/Videos/blue_.mp4  \
              data/Videos/green_.mp4 \
              data/b0.png   \
              data/g0.png   \
              data/Foreground  \
              data/Alpha    \
              data/Videos/final_matte.avi \
              0.01 0 1
```

In the above example, on feeding in `blue_.mp4`,`green_.mp4`,`b0.png` and `g0.png`; with the `video_only` flag set to `false`, implying we do want to output all the intermedite files to the respective given folders and while the processing for that is happening, we also want a live display of the intermedite results (as the `display_output` flag is also set to `true`). As a result, one can obtain the `alpha` per frame within the given `data/Alpha` path and the per frame `foreground` in the given `data/Foreground` path. The collected foreground frames from all of the input are put into an AVI format video in the file `final_matte.avi`.


## Test Suites

Makefile tests are included, to run them traverse to the root of the directory and execute accordingly as given below:

1. `make test-all`:
    Run complete test suite
2. `make video_matte-test`:
    To test the `VideoMatte` capabilties
3. `make video-test`:
    To test `Video` reading and writing
4. `make matte-test`:
    To test `Matte` application on an `Image`
5. `make image-test`:
    To test the capabilies of the `Image` class
6. `make utilities-test`:
    Test capabilites of the `miniUtilities` class
7. `make tester-test`:
    Test capabilites of the `tester` class
8. `make main-test`:
    Test main.cpp
9. `make clean`:
    Cleans everything up, deletes content of `Foreground` and `Alpha` folder and even the `bin` folder

## Directory structure

1. `bin/`: Contains all the binary files
2. `data/`: Contains image files and subfolders `Videos`, `Foregound`, `Alpha` which contain other image and video files
3. `include/trimat`: Contains the header (class definition) in `.h` files
4. `src`: Contains member functions in `.cpp` files
5. `tests`: Contains all the tests

## Classes

I've modeled my own very simple `Image` class which is used by a `Video` class. The ``Video`` class has capabilities to read and write videos all in one place (as opposed to them being separate in OpenCV). That includes releasing the actual object files after use which is very important in a language like C++. I'm also create a Matting (`Matte`) class which relies on the `Image` class to do the Matting. The Matting class is then extended to a `VideoMatte` class (which inherits all the properties of the Matting class) that adds capability to read the blue and green background videos, and write the resultant video (composed of the alphamatte and the foreground image per frame); all utilizing the capabilities of the `Video` class. Although point to note, that this is in no way a critique of the OpenCV library which is in on itself, absolutely amazing and this work only seeks to make the methods which would typically be relevant (or more commonly used in regards to it) to this specific problem (i.e. video foreground and alphamatte estimation) simpler to use and encapsulated closer to the data that it should work on. The generalizability of OpenCV is what makes it powerful in terms of the massive number of features it offers and all I've done is collected and put together some of them for specific use.

Obviously, OpenCV (especially, the `Mat` class and other accompagnying ones which operate on `Mat`) is highly feature rich (as it covers a lot more in terms of capabilities offered across a wide variety of computer vision and even separate pattern recognition tasks, e.g. means to train classical machine learning models on data which might not even have anything to do with computer vision; and if even every capability is not present directly within the `Mat` class -- there's nothing wrong with that, as that allows for near perfect flexibility when using it for other applications) and these are only a select few capabilities which seemed most pertinent. Along these lines, I've also tried to bring in additional simplicity with regards to certain methods (i.e. as compared to using them out of the box from OpenCV vs modified here so that repeated use becomes easier, within this context).

An architecture diagram can be seen below, followed by a brief explanation of each class:

![VideoMatter(2)](https://user-images.githubusercontent.com/28497335/139788267-1ce5df4d-2b57-46c1-9408-94341650979e.png)

1. `Image`: Creates a simple image class, which provides relatively easier access for commonly used OpenCV image functions, e.g. `imshow` is bundled in with the image container rather than separate, images can now be easily allocated from files very simply etc.
2. `Video`: Models OpenCV's `VideoCapture` and `VideoWriter` in one place and simple method calls replace complicated allocation (e.g. `VideoCapture capture;`) and deallocation (`capture.release();`) and it take cares of some other things behind the scenes to make their use simple and painless (e.g. reading videos is much simpler, at least better than typing in massive OpenCV VideoCapture or VideoWriter properties like CAP_PROP_WHITE_BALANCE_BLUE_U, to just quickly get a property or read one frame -- again nothing wrong with that, and sometimes it turned out that one *does* actually need to use a long property for something specific) as well.
3. `Matte`: Does matting using on `Image` objects, i.e. `alpha`mask and `foreground` extraction
4. `VideoMatte`: Extends capabilities of the `Matte` class to handle Videos. Has functionality to contain green and blue videos and even writing any resultant videos (currently only the per frame estimated `foreground` video).
5. `tester`: Provides simple testing capabilities
6. `miniUtilities`: Contains simple string utilities related to filehandling

## Appendix

This section contains a short overview of related concepts.

### Foreground and Images

A **foreground** within an image can be any object or person of interest whom we seek to insert over a separate background or seek to extract, traverse or label for any reason. Anything that is not a foreground is part of the **background**.

![image](https://tipsmake.com/data/images/differences-between-background-and-foreground-picture-1-4vJ1Th9nZ.jpg)

An **image** here can be defined as a 2D matrix of intensities; where the whole image can be thought of composed of these "dots" or pixels which are nothing but having a numerical value. Each value represents the intensity of that pixel and is equivalent to the amount of light falling on the sensor of the camera (as appearing to come in from a given point in the real world) and is hence the brightness of that given pixel.

![abe](https://miro.medium.com/max/700/0*CI5wgSszZnpHu5Ip.png)

This can be single numerical values per pixel or square dot (e.g. in an 8 bit grayscale image as seen above: 8 bit because each value of intensity can be represented by an 8 bit number or have a range of 0-255) or a three numrical color values per pixel (e.g. in a color RGB image, seen below: every channel is like an individual grayscale image, except that the intensities of every channel represent the magnitude of that "color channel", i.e. R, G or B at a given point).

![rgb](https://brohrer.github.io/images/image_processing/reign_pic_breakdown.png)

### Segmentation

A very simple way to extract this foreground from an image could be **segmenting** intentisty values over an image. The most rudimentary form of segmenting involves nothing but setting values within a **binary mask** to one (or zero), for any regions of intensity within the image which are greater or lesser than some numerical threshold e.g. setting all values in a greayscale image (with an intensity range of 0-255) to one, if the values are greater than 127 (pictured below). This can also be termed as segmentation through *thresholding*. The goal being to somehow identify and "segment" or focus out one region from within the image. This can be for something as trivial as localizing or even extracting an object from the image, if the *binary mask* of zeros and ones are relatively clean (to contain only that object).

![segment](https://miro.medium.com/max/700/1*d-mNNgGRQbDtXqXIGHMEPg.png)


So if that object (with a clean binary mask) we're segmenting is a foreground (FG) object then we can do foreground extraction using segmenting (i.e. where-ever the binary mask is one, that's a foreground region in the image, whereever it is zero in the mask, then it is not). This can work say when you're capturing a dark object over a bright background and one knows that any intenstiy value lesser than a certain threshold is the foreground (or the object of interest).

![noise](https://i.imgur.com/RUrQauv.jpg)

But this approach is typically not that robust if one wanted to obtain a clean cut of the foreground (as getting a clean cut of that original segmented object can be hard, especially around the edges) and there can be "noisy" regions, which are essentially not FG regions but still satisfy the thresholding condition defined earlier. Even if these regions are removable through image processing techniques when they're properly separated from the object of interest or the problem to remove this "noise" is a solved one (this can happen say in case of something like the presence of salt and pepper noise or speckles of little dots spread throughout the image), sometimes they're not!

 Moreso, segmenting is not really that good for soft or "fuzzy" edges or localizing objects like feathers which might have regions at the edges which might have components of both some part foreground and background within them at the same time, or that good at handling motion blur, semi-transparent objects, and not really robust to changes in illumination over the image (assuming we're attempting to do this without any additional image processing and advanced segmentation techniques).

 Even advanced techniques for segmentation are not really focused on getting a clean cut of the foreground but rather more interested in *identifying* and the subsequent *marking* of an object of interest. As long as the object is identified and a rough boundary around it is found and we are able to correspond that object to some known infromation if needed be (e.g. past information about position of object), then that would be enough for a segmentation algorithm. This is useful when say, trying to track pedestrians in footage captured from a traffic cam -- we're not really interested in tracing super fine localizations of people around the boundaries of their visible shape forms, but definitely more focused on *accurately identifying* and *marking* even a "rough blob area" over a person in every frame (with accurate correspondences across frames) to track them over the course of the footage.

### Matting

Hence given the drawbacks of segmentation, a more robust way to offput the above inconsitencies (towards finely tracing the outline of the foreground) would be that of attempting to place the foreground on a linear, homogenous background with a color which is easy to differentiate from it (the foreground). We also want to assume that this color is typically not present within the foreground. This is in essence what **matting** does. Single color matting techniques like green or blue screen matting would try to estimate the ***matte*** or boundary of the foreground, in a "soft" or *mixed* fashion by leveraging the fact that the background color is known and seemingly constant throughout.

![matte](https://infocusfilmschool.com/wp-content/uploads/2017/12/mattelessCompositing-705x507.png)

Matting or keying is the problem of estimating a given foreground and the alphamask, which when combined with the foreground and new background can be used to composite (or *blend*) that new foreground onto the given background.

We don't segment or threshold values, rather we solve an equation at every pixel (at most three equations with seven unknowns but with certain assumtions they can be  reduced to three unknowns; seen below for green screen matting) to obtain the values of the FG and alphamask (assuming that the color within the background does not exist within the foreground at all, e.g. greenscreen matting assumes that green does not exist within the foreground and that the background color contains only the green component of color -- that is, no red or blue). The alphamask, `alpha` is nothing but a similar binary mask, **except** that now every dot or pixel has capability to be both foreground and background at the same time (the floating point value per pixel denoting the percentage of how much a pixel could roughly belong to the foreground), i.e. values are floating point and range from zero to one instead of just zero and one (here, values of one imply opaqueness, i.e. certain existance of foreground; while zero implies complete transparency, i.e. no exisitance of foreground).

<p align="center">
<img src="https://i.imgur.com/uzeAZYM.png/"">
</p>


Practically, **green screen matting** can be easily achieved by shooting infront of a green colored background (called a *green screen*; hence the name of the technique) and you might even notice in studio's (especially in the *making of* of various hollywood blockbusters) people shooting infront of massive green screens. That's the foreground (actors) being shot infront of a green screen so that a different background can be inserted later (e.g. The Avengers).


The problem with green screen or even blue screen matting is the lack of capability to handle shadows, the violation of the assumption that background color won't exist in the foregreound in real life, and the illumination from the background "spilling" onto the foreground (this might be prevalent in studios particularly where there are massive green screens).

This is where triangulation matting comes in. It does the same (overlaying of foreground over some constant color) but over two colors instead of one. This leads to six equations instead of three and four unknowns as discussed at the very beginning. **Triangulation matting** is robust to translucent foregrounds, shadows and minimizes other drawbacks of single color (green/blue screen) matting.

Triangulation matting requires that your foregrounds across all images with different backgrounds have the same position and same lighting. You will notice that the test data produced by my colleague (many years ago), in our course project is actually of two frames captured very minutely (a few milliseconds) apart. They're of a game character shooting a rocket but the explosion is captured over a few milliseconds across two frames. The change is so subtle that even the character does not move (from their original location). But, this makes for good test data and you can even see something very interesting in the resulting `alpha` matte itself that there are really subtle and hard to catch inconsistencies right at the explosion spot where the foreground actually differs (slightly across two images) and the foreground trace of the blue image seems to dominate. Otherwise the rest of the image (including trace of the rocket and the character standing) are are still the same and contribute positively to the matte accordingly.


![double](https://i.imgur.com/1ybUBL1.png)


##### Feedback

###### If you notice any inconsitencies or have any input to provide, feel free to reach out to me at dassgaurav93@gmail.com
